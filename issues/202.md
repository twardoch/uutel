Analyze @llms.txt codebase. Itâ€™s supposed to be a Python port of these Vercel AI SDK provider plugins:

- @external/ext/ai-sdk-provider-claude-code.txt
- @external/ext/ai-sdk-provider-gemini-cli.txt
- @external/ext/cloud-code-ai-provider.txt
- @external/ext/codex-ai-provider.txt

Think hard about how good the current code is. 

Check @PLAN.md and @TODO.md for the plan and TODO list.

```sh
$ uutel test
ğŸ§ª Testing engine: my-custom-llm/codex-large
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Verbose mode enabled
ğŸ¯ Using engine: my-custom-llm/codex-large
âš™ï¸  Parameters: max_tokens=50, temperature=0.7
â³ Generating completion...
21:58:06 - LiteLLM:WARNING: utils.py:539 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
Final returned optional params: {'temperature': 0.7, 'max_tokens': 50}
This is a mock response from Codex provider for model codex-large. Received 1 messages. In a real implementation, this would call the actual Codex API.
âœ… Completion successful (151 characters)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Test completed successfully!
ğŸ’¡ Engine 'my-custom-llm/codex-large' is working correctly
This is a mock response from Codex provider for model codex-large. Received 1 messages. In a real implementation, this would call the actual Codex API.
```

We donâ€™t want mock responses. We want actual real responses.

```
$ python ./examples/basic_usage.py
ğŸš€ UUTEL Basic Usage Example
==================================================

1ï¸âƒ£ Model Name Validation
   example-model-1.0: âœ… Valid
   uutel/claude-code/claude-3-5-sonnet: âœ… Valid
   invalid/model: âŒ Invalid
   model with spaces: âŒ Invalid

2ï¸âƒ£ Provider/Model Extraction
   uutel/claude-code/claude-3-5-sonnet â†’ Provider: claude-code, Model: claude-3-5-sonnet
   uutel/gemini-cli/gemini-2.0-flash â†’ Provider: gemini-cli, Model: gemini-2.0-flash
   simple-model â†’ Provider: unknown, Model: simple-model

3ï¸âƒ£ Message Transformation
   Original messages: 2
   After round-trip: 2
   Content preserved: True

4ï¸âƒ£ HTTP Client Creation
   Sync client type: _SyncRetryClient
   Async client type: _AsyncRetryClient

5ï¸âƒ£ Authentication Framework
ğŸ”‘ Authenticating with example using api-key
   âœ… Auth successful: True
   ğŸ”‘ Headers: 2 headers

6ï¸âƒ£ Provider Usage
   ğŸ“¡ Provider: example
   ğŸ¯ Supported models: 2
ğŸ¤– Making completion request to example-model-1.0
ğŸ“ Messages: 2 messages
   ğŸ’¬ Response ID: example-123
   ğŸ“Š Token usage: 25

7ï¸âƒ£ Error Handling
ğŸ”‘ Authenticating with example using api-key
   ğŸš« Caught AuthenticationError: Invalid API key | Provider: example | Code: AUTH_001
   ğŸ“ Provider: example
   ğŸ”¢ Error code: AUTH_001

âœ¨ Example completed successfully!

8ï¸âƒ£ Claude Code Fixture Replay
   ğŸ“ Fixture: tests/data/providers/claude/simple_completion.json
   ğŸ§  Text: Hi! I'm ready to help you with your software engineering tasks. Let me know what you'd like to work on.
   ğŸ“Š Tokens: input=4, output=28, total=32
   ğŸ”„ To run live: npm install -g @anthropic-ai/claude-code && claude login
              uutel complete --engine uutel/claude-code/claude-sonnet-4 --stream

ğŸ”„ Async Functionality Demo
   Created async client: _AsyncRetryClient
   âœ… Async operation completed
```

We actually also want more realistic responses in the examples. Made after probing real possible model names and configs! (Also we want to be able to just select the engine like claude or gemini or codex without specifying the model name!)

Continue to update the /plan and then to /work â€”â€” improve the implementation. We want fully functioning code! 
