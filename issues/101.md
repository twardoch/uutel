Read @external/ext/repo-tldr.txt 

Then analyze: 

- @external/ext/ai-sdk-provider-claude-code.txt 
- @external/ext/ai-sdk-provider-gemini-cli.txt 
- @external/ext/cloud-code-ai-provider.txt 
- @external/ext/codex-ai-provider.txt 

Then into @PLAN.md write a very detailed /plan that creates 'uutel', a Python package that uses the 'litellm' infrastructure to expose LLM inferencing API/Python code based on Claude Code, Gemini CLI and OpenAI Codex. 

The 'ai-provider' files implement such functionality as provider plugins for the Vercel AI SDK. You need to "port" that functionality into Python in a 'litellm'-compatible way. 

Think about the plan, analyze the files, analyze & search & scan the full repos inside @external/repo .  

Once youâ€™ve written the plan, re-analyze the plan, re-analyze & search & scan the full repos inside @external/repo .  

Then improve @PLAN.md and then into @TODO.md write a corresponding flat itemized list of tasks prefixed by - [ ]

